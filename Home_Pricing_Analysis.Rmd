---
title: "Home Price Analysis"
author: "Cindy Cao"
date: "1/15/2019"
output: html_document
code_folding: hide
---

```{r setup, include=FALSE}
gc()
library(data.table)
library(dplyr)
library(weights)
library(ggvis)
library(haven)
library(caret)
library(gbm)
library(utils)
library(Hmisc)
library(woeBinning)
library(lubridate)
library(rpart)
library(gridExtra)
library(grid)
library(caret)
library(gsubfn)
library(sqldf)
library(partykit)
library(knitr)
library(kableExtra)
library(readr)
library(splitstackshape)
library(tidyr)
library(corrplot)
library(car)
library(tree)
library(glmnet)
library(gbm)
library(DMwR)
```

### Outline:
1. Problem Statement
2. Feature Engineering
3. Exploratory Data Analysis
4. Linear Regression
5. Gradient Boosting
6. Conclusion
7. Next Step

### 1. Problem Statement
Home price is a popular and well discussed topic, not only among home buyers, but also among investors, real-estate agencies, online listing platforms, etc. A traditional way to predict home prices is by analyzing house characteritics, such as location, square feet, building year, building materials, etc. However, with the increasing data availabilities, more complex and accurate models utilizing unconventional datasets or modeling techniques are emerging. This project aims to explore the relationship between home prices and various weather observations to better predict home prices.

### 2. Feature Engineering
####Load depository path and functions
```{r, warning=FALSE,message=FALSE}
path<-'/Users/xucao/Google Drive/MyWorkStation/Projects/ODG-Weather-HousingPrice/'
dataloc<-paste0(path,'DATA/')
codeloc<-paste0(path,'CODE/')
output <-paste0(path,'OUTPUT/')
source(paste0(codeloc,"functions.r"))
source(paste0(codeloc,"attr.bivar.R"))             #self defined bivariate plot function
```

####Read in text files \newline

#####read in housing data
```{r, warning=FALSE,message=FALSE}
housing_data<-fread(paste0(dataloc,'housing_data_ACS_15_5YR_DP04_with_ann.csv'),header=TRUE)
#rename zip code column
colnames(housing_data)[2]<-'ZIP'
#convert zip code column to numeric
housing_data$ZIP=as.numeric((housing_data$ZIP))
#drop the top row with variables descriptions
housing_data<-housing_data[2:nrow(housing_data),]
```

#####read in geographic data
```{r, warning=FALSE,message=FALSE}
Gaz_zcta_national<-fread(paste0(dataloc,'2015_Gaz_zcta_national.txt'),header=TRUE)
colnames(Gaz_zcta_national)[1]<-'ZIP'
#convert integer64 columns to numeric 
Gaz_zcta_national$ALAND<-as.numeric(Gaz_zcta_national$ALAND)
Gaz_zcta_national$AWATER<-as.numeric(Gaz_zcta_national$AWATER)
```

#####read in location data
```{r,warning=FALSE,message=FALSE}
weather_allstations<-read_fwf(file=paste0(dataloc,'weather_allstations.txt'),fwf_widths(c(12,9,10,7,3,31,4,4,6)))
#renaming columns
names(weather_allstations)<-c('STATION_ID','LAT','LONG','ELEVATION','STATE','STATION_NAME','SOURCE1','SOURCE2','ZIP_STATION')
#subset useful variables
weather_allstations<-weather_allstations[,c('STATION_ID','ELEVATION','STATE')]
```

#####read in zipcode to weather station mapping table
```{r, warning=FALSE,message=FALSE}
weather_zipcodes_stations<-read_fwf(file=paste0(dataloc,'weather_zipcodes-normals-stations.txt'),fwf_widths(c(12,6,50)))
#Renaming columns
names(weather_zipcodes_stations)<-c('STATION_ID','ZIP','CITY')
#Formatting column 'CITY' and 'ZIP', all 'ZIP' columns are converted to numeric for matching purposes
weather_zipcodes_stations$CITY<-toupper(weather_zipcodes_stations$CITY)
weather_zipcodes_stations$ZIP<-as.numeric(weather_zipcodes_stations$ZIP)
#Create primary key
weather_zipcodes_stations$STATION<-paste0(formatC(weather_zipcodes_stations$ZIP, width=5, flag="0"),toupper(weather_zipcodes_stations$CITY))

```

#####read in weather data
```{r, warning=FALSE,message=FALSE}
namelist<-c('STATION_ID','Weat_Jan','Weat_Jan_Type','Weat_Feb','Weat_Feb_Type','Weat_Mar','Weat_Mar_Type','Weat_Apr','Weat_Apr_Type','Weat_May','Weat_May_Type',
                       'Weat_Jun','Weat_Jun_Type','Weat_Jul','Weat_Jul_Type','Weat_Aug','Weat_Aug_Type','Weat_Sep','Weat_Sep_Type','Weat_Oct','Weat_Oct_Type','Weat_Nov','Weat_Nov_Type',
                       'Weat_Dec','Weat_Dec_Type')

weather_prcp<-read_fwf(file=paste0(dataloc,'weather_mly-prcp-normal.txt'),fwf_widths(c(17,6,1,6,1,6,1,6,1,6,1,6,1,6,1,6,1,6,1,6,1,6,1,6,1)))
weather_tavg<-read_fwf(file=paste0(dataloc,'weather_mly-tavg-normal.txt'),fwf_widths(c(17,6,1,6,1,6,1,6,1,6,1,6,1,6,1,6,1,6,1,6,1,6,1,6,1)))
weather_tmax<-read_fwf(file=paste0(dataloc,'weather_mly-tmax-normal.txt'),fwf_widths(c(17,6,1,6,1,6,1,6,1,6,1,6,1,6,1,6,1,6,1,6,1,6,1,6,1)))
weather_tmin<-read_fwf(file=paste0(dataloc,'weather_mly-tmin-normal.txt'),fwf_widths(c(17,6,1,6,1,6,1,6,1,6,1,6,1,6,1,6,1,6,1,6,1,6,1,6,1)))

names(weather_prcp)<-paste0(namelist,'_prcp')
names(weather_prcp)[1]<-'STATION_ID'
names(weather_tavg)<-paste0(namelist,'_tavg')
names(weather_tavg)[1]<-'STATION_ID'
names(weather_tmax)<-paste0(namelist,'_tmax')
names(weather_tmax)[1]<-'STATION_ID'
names(weather_tmin)<-paste0(namelist,'_tmin')
names(weather_tmin)[1]<-'STATION_ID'

#precipitation cannot be negative, so we convert negative values to NA
for (i in 1:ncol(weather_prcp)){
  if (class(weather_prcp[,i])=='numeric'){
    weather_prcp[,i]<-ifelse(weather_prcp[,i]<0,'NA',weather_prcp[,i])
  }
}


clpcdy15<-separate(read_fwf(file=paste0(dataloc,'clpcdy15.txt'),fwf_widths(c(38,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4)),skip=2),col='X1',into=c('STATION','STATE'),sep=',' )
names(clpcdy15)=c('STATION','STATE','YRS','Jan_CL','Jan_PC','Jan_CD','Feb_CL','Feb_PC','Feb_CD','Mar_CL','Mar_PC','Mar_CD','Apr_CL','Apr_PC','Apr_CD','May_CL','May_PC','May_CD','Jun_CL','Jun_PC','Jun_CD','Jul_CL','Jul_PC','Jul_CD','Aug_CL','Aug_PC','Aug_CD','Sep_CL','Sep_PC','Sep_CD','Oct_CL','Oct_PC','Oct_CD','Nov_CL','Nov_PC','Nov_CD','Dec_CL','Dec_PC','Dec_CD','Ann_CL','Ann_PC','Ann_CD')

#Base on Master Location Identifier Database downloaded from 'http://www.weathergraphics.com/identifiers/', WBAN is the weather station identifiersm. The first column of dataset clpcdy15 and pctpos15 consists of WBAN and CITY name.
clpcdy15$WBAN<-as.numeric(substr(clpcdy15$STATION,1,5))
MLID<-fread(file=paste0(dataloc,'MLID.csv'),header=TRUE)
MLID<-MLID%>%select(WBAN, (CITY))
MLID$CITY<-toupper(MLID$CITY)
clpcdy15<-clpcdy15%>%left_join(MLID,by='WBAN')
clpcdy15$CITY<-coalesce(clpcdy15$CITY,substr(clpcdy15$STATION, 6, 50))
clpcdy15$STATION=paste0(clpcdy15$WBAN,clpcdy15$CITY)
   
pctpos15<-separate(read_fwf(file=paste0(dataloc,'pctpos15.txt'),fwf_widths(c(37,16,6,6,6,6,6,6,6,6,6,6,6,6,6)),skip=2),col='X1',into=c('STATION','STATE'),sep=',' )   
names(pctpos15)=c('STATION','STATE','POR','JAN','FEB','MAR','APR','MAY','JUN','JUL','AUG','SEP','OCT','NOV','DEC','ANN')
pctpos15$WBAN<-as.numeric(substr(pctpos15$STATION,1,5))
pctpos15$CITY<-substr(pctpos15$STATION, 6, 50)

```

####Check variable datatype
```{r , echo = T, results = 'hide', warning=FALSE}
sapply(housing_data, class) 
#We only keep the actual values (HC01), and drop other statistics columns, such as margin of error, percentage, etc.
housing_data<-housing_data%>%select(ZIP,starts_with('HC01_'))
#convert to numerica data
housing=as.data.frame(sapply(housing_data, as.numeric))
sapply(Gaz_zcta_national, class) 
sapply(weather_allstations, class) 
weather_allstations$ELEVATION<-as.numeric(weather_allstations$ELEVATION)
sapply(weather_zipcodes_stations, class) 
sapply(weather_prcp, class) 
sapply(weather_tavg, class) 
#convert character tempurature data into numeric data
weather_tavg$Weat_Jul_tavg<-as.numeric(weather_tavg$Weat_Jul_tavg)
weather_tavg$Weat_Aug_tavg<-as.numeric(weather_tavg$Weat_Aug_tavg)
sapply(weather_tmax, class) 
#convert character tempurature data into numeric data
weather_tmax$Weat_May_tmax<-as.numeric(weather_tmax$Weat_May_tmax)
weather_tmax$Weat_Jun_tmax<-as.numeric(weather_tmax$Weat_Jun_tmax)
weather_tmax$Weat_Jul_tmax<-as.numeric(weather_tmax$Weat_Jul_tmax)
weather_tmax$Weat_Aug_tmax<-as.numeric(weather_tmax$Weat_Aug_tmax)
weather_tmax$Weat_Sep_tmax<-as.numeric(weather_tmax$Weat_Sep_tmax)
sapply(weather_tmin, class) 
sapply(clpcdy15, class) 
clpcdy15<-cbind(clpcdy15[1:3], lapply(clpcdy15[4:42], as.numeric) ,clpcdy15[43:44])
sapply(pctpos15, class) 
```

####Check datasets primary key duplicates
```{r, echo = T, results = 'hide', warining=FALSE}
clpcdy15$STATION[duplicated(clpcdy15$STATION)]
#dedup on primary key
clpcdy15<-clpcdy15[!duplicated(clpcdy15$STATION),]
pctpos15$STATION[duplicated(pctpos15$STATION)]
pctpos15<-pctpos15[!duplicated(pctpos15$STATION),]
housing$ZIP[duplicated(housing$ZIP)]
Gaz_zcta_national$ZIP[duplicated(Gaz_zcta_national$ZIP)]
weather_allstations$STATION_ID[duplicated(weather_allstations$STATION_ID)]
weather_zipcodes_stations$STATION_ID[duplicated(weather_zipcodes_stations$STATION_ID)]
weather_prcp$STATION_ID[duplicated(weather_prcp$STATION_ID)]
weather_tavg$STATION_ID[duplicated(weather_tavg$STATION_ID)]
weather_tmax$STATION_ID[duplicated(weather_tmax$STATION_ID)]
weather_tmin$STATION_ID[duplicated(weather_tmin$STATION_ID)]

```

####Merge datasets
```{r , warning=FALSE,message=FALSE}
data<-weather_zipcodes_stations%>%left_join(weather_allstations,by='STATION_ID')%>%left_join(weather_prcp,by='STATION_ID')%>%left_join(weather_tavg,by='STATION_ID')%>%left_join(weather_tmax,by='STATION_ID')%>%left_join(weather_tmin,by='STATION_ID')%>%left_join(housing,by='ZIP')%>%left_join(Gaz_zcta_national,by='ZIP')%>%left_join(pctpos15,by='STATION')%>%left_join(clpcdy15,by='STATION')
```

####Examine target
```{r, warning=FALSE}
summary(data$HC01_VC128)
#drop records with NA target, these records without target informaiton are not useful to our supervised learning analysis.
data<-data[!is.na(data$HC01_VC128),]

#simulate gamma distribution 
theta=(sd(log(data$HC01_VC128)))^2/mean(log(data$HC01_VC128))
k=mean(log(data$HC01_VC128))/theta
gamma<-as.data.frame(rgamma(nrow(data),shape=k,scale=theta))
gamma$source<-'gamma'
names(gamma)=c('value','source')
#simulate normal distribution 
mean<-mean(log(data$HC01_VC128))
sd<-sd(log(data$HC01_VC128))
norm<-as.data.frame(rnorm(nrow(data),mean=mean,sd=sd))
norm$source='norm'
names(norm)=c('value','source')
#plot actual dependent variable distribution 
actual<-as.data.frame(log(data$HC01_VC128))
actual$source='actual'
names(actual)=c('value','source')
dist<-rbind(gamma,norm,actual)
dist$value<-as.numeric(dist$value)
ggplot(dist,aes(value, fill=source))+geom_density(alpha=0.5)

#based on the above analysis, by comparing the distribution shape, a log normal distribution will be used for the target.
data$Target<-log(data$HC01_VC128)

```

####Split Train/Test/Holdout: 50%/25%/25% \newline

This step is to prepare data for future modeling practice, and we only conduct featue engineering and exploratory analysis on the training set. 
```{r cars,warning=FALSE}
set.seed(1)
inTrainingSet = createDataPartition(log(data$HC01_VC128),p=.5, list = FALSE)
dt = data[inTrainingSet,]
trainset = data[-inTrainingSet,]
set.seed(123)
inTrainingSet = createDataPartition(log(dt$HC01_VC128),p=.5, list = FALSE)
holdout = dt[inTrainingSet,]
testset = dt[-inTrainingSet,]

gbm.trainset<-trainset
gbm.testset<-testset
```

Check dependent variable distributions in different datasets. We are looking for similar dependent variable distributions among train/test/holdout sets to ensure that we do not have biased sample selection.
```{r, warning=FALSE}
##train number of records:
nrow(trainset)
summary(log(trainset$HC01_VC128))
train_target_dist<-cbind(log(trainset$HC01_VC128),'trainset')
##test number of records:
nrow(testset)
summary(log(testset$HC01_VC128))
test_target_dist<-cbind(log(testset$HC01_VC128),'testset')
##holdout number of records:
nrow(holdout)
summary(log(holdout$HC01_VC128))
holdout_target_dist<-cbind(log(holdout$HC01_VC128),'holdout')

target_dist<-as.data.frame(rbind(train_target_dist,test_target_dist,holdout_target_dist))
names(target_dist)<-c('value','source')
target_dist$value<-as.numeric(target_dist$value)
ggplot(target_dist,aes(value, fill=source))+geom_density(alpha=0.5)
```

Before imputing missing values, we create missing flags and cap outliers first, as missing flags carry meaningful information, and outliers can impact missing value imputation.

####Create missing flags

```{r, warning=FALSE,message=FALSE}
for (i in 1:ncol(trainset)){
  if(colnames(trainset)[i]!='HC01_VC128' & colnames(trainset)[i]!='Target'){
    trainset[,paste0("flag_",colnames(trainset)[i])]<-as.factor((ifelse(is.na(trainset[,i]),1,0)))
  }
}

```

####Cap outliers \newline

Due to the constrained time, here we use 5% and 95% quantiles to cap all the attributes. For future study, we can run chi-square outlier test and treat each attribute separately for outlier capping.\newline
```{r, warning=FALSE, message=FALSE, fig.height=4,fig.width=6}
#boxplot shows outliers for dependent variable
ggplot(data=trainset,aes(x=1,y=trainset$HC01_VC128))+geom_boxplot()+xlab('Boxplot')+ylab('')

#tranform and cap dependent variable
trainset$Target<-pmin(log(trainset$HC01_VC128),quantile(log(trainset$HC01_VC128),0.99))
trainset<-trainset%>%select(-HC01_VC128)

#cap numeric independent variable
for (i in 1:ncol(trainset)){
if ((substr(colnames(trainset)[i],1,5)=='Weat_'|substr(colnames(trainset)[i],1,5)=='HC01_')&(class(eval(parse(text=paste0('trainset$',colnames(trainset)[i]))))=='numeric'|class(eval(parse(text=paste0('trainset$',colnames(trainset)[i]))))=='integer')){
  trainset[,paste0(colnames(trainset)[i],'_cap')]<-pmin(pmax(eval(parse(text=paste0('trainset$',colnames(trainset)[i]))),quantile(eval(parse(text=paste0('trainset$',colnames(trainset)[i]))),0.05,na.rm=TRUE)),quantile(eval(parse(text=paste0('trainset$',colnames(trainset)[i]))),0.95,na.rm=TRUE))
}
}

#save a capping value table for testset transformation later
trainset.num<-trainset[,sapply(trainset,class)=='numeric'|sapply(trainset,class)=='integer']
capping<-as.matrix(rbind(colnames(trainset.num),sapply(trainset.num,function(x) quantile(x,0.05,na.rm=TRUE)),sapply(trainset.num,function(x) quantile(x,0.95,na.rm=TRUE))))
colnames(capping)<-capping[1,]
capping<-as.data.frame(capping[2:3,])
```

####Impute missing values \newline

Regarding to missing values, we treat variables from different datasets differently. \newline

1. For weather related variables, climate is highy correlated with location, and areas within the same state share similar climate, therefore one way to impute the weather data is to use the state mean, and we have fully populated state information in the dataset, which makes this approach plausible. \newline

2. For house related variables, we can try two different approches, one is to utilize bivariate plot to go through the relationships between each indenpent variable and independent variables, and impute the missing values with user defined statistical method (mean, min, max, zero, etc.), or we can use k nearest neighor to impute the missing values. Here we use k-nearest neighbor to impute our housing variables.\newline

Select useful columns
```{r, warning=FALSE}
trainset<-as.data.frame(trainset%>%select(Target,ends_with('_cap'),starts_with('flag_'),contains('_Type'),ELEVATION,STATE.x,ALAND,AWATER))
```

Seperate different data type columns
```{r, warning=FALSE}
colclasses <- sapply(trainset,class)
numColumns <- which(colclasses=="numeric"|colclasses=="integer")
JustNumbers<-trainset%>%select(STATE.x,numColumns)
```

Create state average lookup table for weather related attributes imputation
```{r, warning=FALSE}
weat_state<-list()
for (i in seq_along(1:(ncol(JustNumbers)))){
  if (i==1){
   weat_state[[i]]<-as.vector((JustNumbers%>%select(STATE.x,colnames(JustNumbers)[i])%>%group_by(STATE.x)%>%summarise(avg=mean(eval(parse(text=colnames(JustNumbers)[i])),na.rm=TRUE)))[,1])  
  } else{
  weat_state[[i]]<-as.vector((JustNumbers%>%select(STATE.x,colnames(JustNumbers)[i])%>%group_by(STATE.x)%>%summarise(avg=mean(eval(parse(text=colnames(JustNumbers)[i])),na.rm=TRUE)))[,2])
  }
}

weat_state_avg<-matrix(nrow=51)
for (i in 1:length(weat_state)){
  weat_state_avg<-cbind(weat_state_avg,(unlist(weat_state[i])))
}
weat_state_avg<-as_tibble((weat_state_avg))[,2:ncol(weat_state_avg)]
weat_state_avg<-cbind(weat_state_avg[,1],sapply(weat_state_avg[,2:ncol(weat_state_avg)],as.numeric))
names(weat_state_avg)=c('STATE.x',paste0('state_avg_',colnames(JustNumbers)[2:ncol(JustNumbers)]))

trainset<-as.data.frame(trainset%>%left_join(weat_state_avg,by='STATE.x'))
```

Attributes imputation
```{r, warning=FALSE}
#Impute house characteristics data first with k nearest neighbor
HC_01<-trainset%>%select(starts_with('HC01_'))
HC_01_Imputed<-knnImputation(HC_01,k=3)
names(HC_01_Imputed)<-paste0('Impute_',colnames(HC_01_Imputed))
  
#Imput weather factor data with mode and numeric data with state average
for (i in 1:ncol(trainset)){
  if((class(trainset[,i])=='character'|class(trainset[,i])=='factor')&substr(colnames(trainset)[i],1,5)!='flag_'){   #Impute Weather type data and factors
    trainset[,i][trainset[,i] == 'P'] <- NA
    trainset[,paste0("F_",colnames(trainset)[i])]<-impute(as.factor(trainset[,i]),mode)
  }
  else if((substr(colnames(trainset)[i],1,5)=='Weat_')&(class(trainset[,i])=='numeric'|class(trainset[,i])=='integer')&(substr(colnames(trainset)[i],1,10)!='state_avg_')){ #Impute Weather numeric data
    trainset[,i]<-as.numeric(trainset[,i])
    trainset[,paste0("Impute_",colnames(trainset)[i])]=coalesce(trainset[,i],eval(parse(text=paste0('trainset$state_avg_',colnames(trainset)[i]))))#Impute state average
  }
}

trainset<-cbind(trainset,HC_01_Imputed)
trainset<-as.data.frame(trainset%>%select(Target,starts_with('flag_'),starts_with('Impute_'),starts_with('F_'),ELEVATION, ALAND, AWATER))
```

####Create climate regions \newline

Climate is highly impacted by the geograpic feature of the location. However, how granular our data needs to be regarding to geographic feature is debatable. In this project, we include both State and Climate regions information in our model. 
```{r, warning=FALSE,message=FALSE}
trainset$Regions<-as.factor(ifelse(trainset$F_STATE.x%in%c('NT','WY','CO','ND','SD','NE','KS'),'northwest',ifelse(trainset$F_STATE.x%in%c('MN','IA','IL','WI','MO','OH','IN','MI'),'midwest',ifelse(trainset$F_STATE.x%in%c('AZ','NM','TX','OK'),'southwest',ifelse(trainset$F_STATE.x%in%c('WA','OR','NV','ID','UT','CA'),'westcoast',ifelse(trainset$F_STATE.x%in%c('AR','LA','MS','TN','AL','GA','FL','SC','NC','VA','WV','KY'),'south',('northeast')))))))

# trainset<-trainset%>%select(-F_STATE.x,)
```

###3. Exploratory Analysis \newline

After feature engineering, now we can look at some of the important weather attributes, and examine their relationships with home price.

#### precipitation
```{r, warning=FALSE,message=FALSE, fig.height=4,fig.width=6}
#spring months
bivar.plot(trainset,'Impute_Weat_Apr_prcp_cap','Target',n.rank=50)
bivar.plot(trainset,'Impute_Weat_May_prcp_cap','Target',n.rank=50)
#summer months
bivar.plot(trainset,'Impute_Weat_Jun_prcp_cap','Target',n.rank=50)
#fall months
bivar.plot(trainset,'Impute_Weat_Sep_prcp_cap','Target',n.rank=50)
#winter months
bivar.plot(trainset,'Impute_Weat_Jan_prcp_cap','Target',n.rank=50)
bivar.plot(trainset,'Impute_Weat_Feb_prcp_cap','Target',n.rank=50)
```

From the above charts, we see that higher precipitation areas has lower home price in summer months, however higher home price in winter months. One interesting observation about this attribute is that in September, precipitation has a nonlinear relationship with home price.  

To deal with this relationship, we can create two variables to represent different trends in different range. Here we use WOE to find the cutoff point.

WOE formula:

WOE=ln($\frac{p(non-event)}{p(event)}$)

IV=$\sum_{i=1}^n (DistributionGood-DistributionBad)$*WOE

```{r, warning=FALSE, message=FALSE, fig.height=4,fig.width=6}
WOE_numeric_split(x='Impute_Weat_Sep_prcp_cap',y1='Target',data=trainset,group=10)
```

Seen from the above WOE graph, the trend changed from negative to positive at bin 4, and we can find the mean of the independent variable for bin 4 from the table, which is -0.08290098. So we use this value as our cutoff value.

```{r}
trainset$Impute_Weat_Sep_prcp_g1_cap<-pmax(-0.08290098-trainset$Impute_Weat_Sep_prcp_cap,0)
trainset$Impute_Weat_Sep_prcp_g2_cap<-pmax(trainset$Impute_Weat_Sep_prcp_cap+0.08290098,0)

trainset<-trainset%>%select(-Impute_Weat_Sep_prcp_cap)
```

#### average tempurature
```{r, warning=FALSE,message=FALSE, fig.height=4,fig.width=6}
#spring months
bivar.plot(trainset,'Impute_Weat_Apr_tavg_cap','Target',n.rank=50)
#summer months
bivar.plot(trainset,'Impute_Weat_Jun_tavg_cap','Target',n.rank=50)
bivar.plot(trainset,'Impute_Weat_Jul_tavg_cap','Target',n.rank=50)
bivar.plot(trainset,'Impute_Weat_Aug_tavg_cap','Target',n.rank=50)
#winter months
bivar.plot(trainset,'Impute_Weat_Jan_tavg_cap','Target',n.rank=50)
```

From the above charts, we see that home price is higher when summer months tempurature is lower, winter months tempurature does not have as big of an impact to the home price as summer tempurater.

#### maximum tempurature
```{r, warning=FALSE, message=FALSE, fig.height=4,fig.width=6}
#spring months
bivar.plot(trainset,'Impute_Weat_Apr_tmax_cap','Target',n.rank=50)
bivar.plot(trainset,'Impute_Weat_May_tmax_cap','Target',n.rank=50)
#summer months
bivar.plot(trainset,'Impute_Weat_Jun_tmax_cap','Target',n.rank=50)
bivar.plot(trainset,'Impute_Weat_Jul_tmax_cap','Target',n.rank=50)
bivar.plot(trainset,'Impute_Weat_Aug_tmax_cap','Target',n.rank=50)
#winter months
bivar.plot(trainset,'Impute_Weat_Dec_tmax_cap','Target',n.rank=50)
bivar.plot(trainset,'Impute_Weat_Jan_tmax_cap','Target',n.rank=50)
```

#### minimum tempurature
```{r, warning=FALSE, message=FALSE, fig.height=4,fig.width=6}
#spring months
bivar.plot(trainset,'Impute_Weat_May_tmin_cap','Target',n.rank=50)
#summer months
bivar.plot(trainset,'Impute_Weat_Jun_tmin_cap','Target',n.rank=50)
bivar.plot(trainset,'Impute_Weat_Jul_tmin_cap','Target',n.rank=50)
bivar.plot(trainset,'Impute_Weat_Aug_tmin_cap','Target',n.rank=50)
#winter months
bivar.plot(trainset,'Impute_Weat_Dec_tmin_cap','Target',n.rank=50)
bivar.plot(trainset,'Impute_Weat_Jan_tmin_cap','Target',n.rank=50)
```

#### regions
```{r, warning=FALSE,message=FALSE}
trainset%>%group_by(Regions)%>%summarise(avg_home_price=mean((Target)))
```

Based on the exploratory analysis, we can see that geographic location, house characteristics and weather data are all predictable. The home price varies depending on which state the house is located, the house characteristics, and the location tempurature and the percentage of snowfall.

#### recording type
As for each record, the recording type is the same for weather attributes, so here we only look at one recording type attribute as an example.
```{r, warning=FALSE,message=FALSE}
trainset%>%group_by(F_Weat_Jan_Type_prcp)%>%summarise(avg_home_price=mean((Target)))
trainset%>%group_by(F_Weat_Jan_Type_tavg)%>%summarise(avg_home_price=mean((Target)))
trainset%>%group_by(F_Weat_Jan_Type_tmax)%>%summarise(avg_home_price=mean((Target)))
trainset%>%group_by(F_Weat_Jan_Type_tmin)%>%summarise(avg_home_price=mean((Target)))
```

###4. Linear Regression
Standardize numeric attributes \newline

As our weather and house data have very different scales, to better compare the coefficients of our models, we standardize the attributes before running models.
```{r, warning=FALSE,message=FALSE}
colclasses <- sapply(trainset,class)
numColumns <- which(colclasses=="numeric"|colclasses=="integer")
JustNumbers<-trainset%>%select(numColumns)

preObj <- preProcess(JustNumbers[, -1], method=c("center", "scale"))
scaled.JustNumbers <- predict(preObj, JustNumbers[, -1])

othrColumns<-trainset%>%select(-numColumns)

trainset<-cbind(trainset$Target,othrColumns,scaled.JustNumbers)
names(trainset)[1]<-'Target'
```

Before variable reduction, we create dummy variables for all the factor columns first.
```{r, warning=FALSE,message=FALSE}
trainset<-Filter(function(x)(length(unique(x))>1), trainset)
x <- model.matrix(Target~., trainset)[,-1]
y<-trainset$Target

z<-as.data.frame(x)
colnames(z)<-make.names(names(as.data.frame(z)), unique = FALSE, allow_ = TRUE)
trainset<-cbind(y,as.data.frame(z))
names(trainset)[1]<-'Target'
```

####Variable reduction
Lasso regression
```{r, warning=FALSE,message=FALSE}
trainset<-Filter(function(x)(length(unique(x))>1), trainset)

set.seed(123)
cv.lasso <- cv.glmnet(x, (y), type.measure='mse',nfolds=5, alpha = 1, family = "gaussian")
cv.lasso$lambda.min

model <- glmnet(x, y, alpha = 1, family = "gaussian",
                lambda = cv.lasso$lambda.min)
print(model)
rank<-caret::getModelInfo("glmnet")$glmnet$varImp(model,lambda=cv.lasso$lambda.min)

#check variables with 0 coefficients
c<-coef(cv.lasso,s='lambda.min',exact=TRUE)
inds<-which(c==0)
variables<-row.names(c)[sort(inds,decreasing = FALSE)]
print(variables[1:50])

```
From Lasso regression, we see that most of the variables with 0 coefficients are flag variables. This gives us a general idea on what variables to drop later.

####Linear regression
#####Backward Selection
```{r echo=TRUE,results='hide', warning=FALSE,message=FALSE}
trainset=trainset[!is.na(trainset$Target),]
#drop no variation columns
trainset<-Filter(function(x)(length(unique(x))>1), trainset)

#model_1
base.model<-glm(Target~., data=trainset, family = 'gaussian')

#Get variable list with P-value less than 0.05
varslist<-summary(base.model)$coefficients[(summary(base.model)$coef[, "Pr(>|t|)"])<0.05&!is.na((summary(base.model)$coef[, "Pr(>|t|)"])<0.05),1]

#model_2
trainset2<-trainset[,c(names(varslist)[2:length(names(varslist))])]
trainset2<-cbind(trainset$Target,trainset2)
names(trainset2)[1]<-'Target'
base.model2<-glm(Target~., data=trainset2, family = 'gaussian')

#Get variable list with P-value less than 0.05
varslist2<-summary(base.model2)$coefficients[(summary(base.model2)$coef[, "Pr(>|t|)"])<0.05,1]
names(varslist2)
vif(lm(base.model2, data=trainset2))

#model_3
trainset3<-trainset2[,c(names(varslist2)[2:length(names(varslist2))])]
trainset3<-cbind(trainset$Target,trainset3)
names(trainset3)[1]<-'Target'
base.model3<-glm(Target~., data=trainset3, family = 'gaussian')
vif(lm(base.model3, data=trainset3))

```

```{r, warning=FALSE,message=FALSE}
#model_4
#Drop variables with high vif values in base.model3 
trainset4<-trainset3%>%select(-Impute_HC01_VC03_cap,-Impute_Weat_Oct_tmax_cap,-Impute_HC01_VC141_cap,-flag_HC01_VC691)
base.model4<-glm(Target~., data=trainset4, family = 'gaussian')
summary(base.model4)
```

#####Check multicollinearity with variance inflation factor
```{r, warning=FALSE, message=FALSE}
vif(lm(base.model4, data=trainset4))
```

We do not see extremly high vif values. Therefore, our final linear regression model does not have multi-collinearity issue.

####Getting ready for preformance test
#####Testset transformation - the same transformations as trainset  
*For any transformation explanations, refer to previous trainset feature engineering section.
```{r, warning=FALSE,message=FALSE}
for (i in 1:ncol(testset)){
  if(colnames(testset)[i]!='HC01_VC128' & colnames(testset)[i]!='Target'){
    testset[,paste0("flag_",colnames(testset)[i])]<-as.factor((ifelse(is.na(testset[,i]),1,0)))
  }
}

testset<-as.data.frame(testset%>%select(-HC01_VC128))

#capping variables using trainset capping values
for (i in 1:ncol(testset)){
  if ((substr(colnames(testset)[i],1,5)=='Weat_'|substr(colnames(testset)[i],1,5)=='HC01_')&(class(eval(parse(text=paste0('testset$',colnames(testset)[i]))))=='numeric'|class(eval(parse(text=paste0('testset$',colnames(testset)[i]))))=='integer')){
    testset[,paste0(colnames(testset)[i],'_cap')]<-pmin(pmax(testset[,i],capping[1,colnames(testset)[i]]),capping[2,colnames(testset)[i]])
  }
}

testset<-as.data.frame(testset%>%select(Target,ends_with('_cap'),starts_with('flag_'),contains('_Type'),STATE.x,ELEVATION))

testset<-as.data.frame(testset%>%left_join(weat_state_avg,by='STATE.x'))

#Impute house characteristics data first with k nearest neighbor
HC_01<-testset%>%select(starts_with('HC01_'))
HC_01_Imputed<-knnImputation(HC_01,k=3)
names(HC_01_Imputed)<-paste0('Impute_',colnames(HC_01_Imputed))

#Imput weather data with mode and state average
for (i in 1:ncol(testset)){
  if((class(testset[,i])=='character'|class(testset[,i])=='factor')&substr(colnames(testset)[i],1,5)!='flag_'){   #Impute Weather type data and factors
    testset[,paste0("F_",colnames(testset)[i])]<-impute(as.factor(testset[,i]),mode)
  }
  else if((substr(colnames(testset)[i],1,5)=='Weat_')&(class(testset[,i])=='numeric'|class(testset[,i])=='integer')&(substr(colnames(testset)[i],1,10)!='state_avg_')){ #Impute Weather numeric data
    testset[,i]<-as.numeric(testset[,i])
    testset[,paste0("Impute_",colnames(testset)[i])]=coalesce(testset[,i],eval(parse(text=paste0('testset$state_avg_',colnames(testset)[i]))))#Impute state average
  }
}

testset<-cbind(testset,HC_01_Imputed)
testset<-as.data.frame(testset%>%select(Target,starts_with('flag_'),starts_with('Impute_'),starts_with('F_'),ELEVATION))

testset$Regions<-as.factor(ifelse(testset$F_STATE.x%in%c('NT','WY','CO','ND','SD','NE','KS'),'northwest',ifelse(testset$F_STATE.x%in%c('MN','IA','IL','WI','MO','OH','IN','MI'),'midwest',ifelse(testset$F_STATE.x%in%c('AZ','NM','TX','OK'),'southwest',ifelse(testset$F_STATE.x%in%c('WA','OR','NV','ID','UT','CA'),'westcoast',ifelse(testset$F_STATE.x%in%c('AR','LA','MS','TN','AL','GA','FL','SC','NC','VA','WV','KY'),'south',('northeast')))))))

# testset<-testset%>%select(-F_STATE.x)
testset$Impute_Weat_Sep_prcp_g1_cap<-pmax(-0.08290098-testset$Impute_Weat_Sep_prcp_cap,0)
testset$Impute_Weat_Sep_prcp_g2_cap<-pmax(testset$Impute_Weat_Sep_prcp_cap+0.08290098,0)

testset<-testset%>%select(-Impute_Weat_Sep_prcp_cap)

colclasses <- sapply(testset,class)
numColumns <- which(colclasses=="numeric"|colclasses=="integer")
JustNumbers<-testset%>%select(numColumns)

preObj <- preProcess(JustNumbers[, -1], method=c("center", "scale"))
scaled.JustNumbers <- predict(preObj, JustNumbers[, -1])

OthrNumbers<-testset%>%select(-numColumns)

set<-cbind(testset$Target,OthrNumbers,scaled.JustNumbers)
names(set)[1]<-'Target'

testset<-Filter(function(x)(length(unique(x))>1), testset)
# testset$Target<-log(testset$Target)

x <- model.matrix(Target~., testset)[,-1]
y<-testset$Target

x<-as.data.frame(x)
colnames(x)<-make.names(names(as.data.frame(x)), unique = FALSE, allow_ = TRUE)
testset<-cbind(y,as.data.frame(x))
names(testset)[1]<-'Target'


```

#####Predict home price on testset
```{r, warning=FALSE,message=FALSE}
testset$glm_pred<-round(predict(base.model4,newdata=testset))
```

#####Rescale prediction to the dependent variable mean \newline

As we standardized our independent variables to mean=0, we need to rescale our prediction back to the actual dependent variable mean level for the next step.
```{r, warning=FALSE,message=FALSE}
testset$glm_pred_rescaled<-(testset$glm_pred)*(mean(exp(testset$Target))/mean(testset$glm_pred))
```

#####Plot Gains Chart \newline

*GAINS.CHART function is a self defined function. The function uses predicted dependent variables quantiles as x-axis, and plot the actual dependent variable on y-axis. A good gains chart should show a nice upward trend with good KS value, and good percentage gains between the highest group and the lowest group.
```{r, warning=FALSE,message=FALSE}
GAINS.CHART((exp(testset$Target)),(testset$glm_pred_rescaled),n.rank=20)
```

####Lorenz Curve, Train vs Test

Lorenz Curve is commonly used to check model overfitting
```{r, warning=FALSE, message=FALSE}
#predict and rescale on trainset
trainset4$glm_pred<-round(predict(base.model4,newdata=trainset4))
trainset4$glm_pred_rescaled<-(trainset4$glm_pred)*(mean(exp(trainset4$Target))/mean(trainset4$glm_pred))

##Lorenz Curve
trainCheck<- OrderedCDF(Score = trainset4$glm_pred, Loss = exp(trainset4$Target), Weight =rep(1,nrow(trainset4)), NBins = 1000)
ValidationCheck<- OrderedCDF(Score = testset$glm_pred, Loss = exp(testset$Target), Weight =rep(1,nrow(testset)), NBins = 1000)

ValidationWeight <- ValidationCheck$WeightPts
ValidationLoss <- ValidationCheck$LossPts
TrainWeight <- trainCheck$WeightPts
TrainLoss <- trainCheck$LossPts

ModelComparisonData <- data.frame(ValidationWeight, ValidationLoss, TrainWeight, TrainLoss)

```

```{r warning=FALSE, message=FALSE, fig.height=4,fig.width=6}
ModelComparisonData %>% ggvis(~ValidationWeight, ~ValidationLoss) %>% layer_paths(stroke := "blue") %>% 
  layer_paths(data = ModelComparisonData, x = ~ValidationWeight, y = ~ValidationWeight, stroke := "black") %>%
  layer_paths(data = ModelComparisonData, x = ~TrainWeight, y = ~TrainLoss, stroke := "red")
```

The above chart shows lorenz curve for Trainset (redline) and Testset (blueline). Further away the curves are from the diagonal line means better prediction power. From the chart, our testset performance was even better than trainset, therefore, no overfitting problem presents in the linear regression model.

###5. Gradient boosting

####Trainset Variable Transformation for gradient boosting
```{r warning=FALSE,message=FALSE, fig.height=4,fig.width=6}
#Train set
#Create missing flags
for (i in 1:ncol(gbm.trainset)){
  if(colnames(gbm.trainset)[i]!='HC01_VC128' & colnames(gbm.trainset)[i]!='Target'){
    gbm.trainset[,paste0("flag_",colnames(gbm.trainset)[i])]<-as.factor((ifelse(is.na(gbm.trainset[,i]),1,0)))
  }
}

#boxplot shows outliers for dependent variable
ggplot(data=gbm.trainset,aes(x=1,y=gbm.trainset$HC01_VC128))+geom_boxplot()+xlab('Boxplot')+ylab('')
#tranform and cap dependent variable
gbm.trainset$Target<-pmin(gbm.trainset$HC01_VC128,quantile(gbm.trainset$HC01_VC128,0.99))
gbm.trainset$Target<-gbm.trainset$HC01_VC128
gbm.trainset<-as.data.frame(gbm.trainset%>%select(-HC01_VC128))

#Remove independent variable outliers
for (i in 1:ncol(gbm.trainset)){
if ((substr(colnames(gbm.trainset)[i],1,5)=='Weat_'|substr(colnames(gbm.trainset)[i],1,5)=='HC01_')&(class(eval(parse(text=paste0('gbm.trainset$',colnames(gbm.trainset)[i]))))=='numeric'|class(eval(parse(text=paste0('gbm.trainset$',colnames(gbm.trainset)[i]))))=='integer')){
  gbm.trainset[,i]<-pmin(pmax(eval(parse(text=paste0('gbm.trainset$',colnames(gbm.trainset)[i]))),quantile(eval(parse(text=paste0('gbm.trainset$',colnames(gbm.trainset)[i]))),0.05,na.rm=TRUE)),quantile(eval(parse(text=paste0('gbm.trainset$',colnames(gbm.trainset)[i]))),0.95,na.rm=TRUE))
}
}

#save a capping value table for gbm.testset transformation later
gbm.trainset.num<-gbm.trainset[,sapply(gbm.trainset,class)=='numeric'|sapply(gbm.trainset,class)=='integer']
capping.gbm<-as.matrix(rbind(colnames(gbm.trainset.num),sapply(gbm.trainset.num,function(x) quantile(x,0.05,na.rm=TRUE)),sapply(gbm.trainset.num,function(x) quantile(x,0.95,na.rm=TRUE))))
colnames(capping.gbm)<-capping.gbm[1,]
capping.gbm<-as.data.frame(capping.gbm[2:3,])

#Convert character variables to factors
for (i in 1:ncol(gbm.trainset)){
  if(class(gbm.trainset[,i])=="character"){   
    gbm.trainset[,i]<-as.factor(gbm.trainset[,i])
  } else {
    gbm.trainset[,i]=gbm.trainset[,i]
  }
}

#select useful columns 
gbm.trainset<-as.data.frame(gbm.trainset%>%select(Target,starts_with('Weat_'),starts_with('flag_'),starts_with('HC01_'),STATE.x,ELEVATION))

```

####Testset Variable Transformation for gradient boosting
```{r warning=FALSE, message=FALSE}
#Create missing flags
for (i in 1:ncol(gbm.testset)){
  if(colnames(gbm.testset)[i]!='HC01_VC128' & colnames(gbm.testset)[i]!='Target'){
    gbm.testset[,paste0("flag_",colnames(gbm.testset)[i])]<-as.factor((ifelse(is.na(gbm.testset[,i]),1,0)))
  }
}

#Create Target variable
gbm.testset$Target<-gbm.testset$HC01_VC128
gbm.testset<-as.data.frame(gbm.testset%>%select(-HC01_VC128))

#capping variables using gbm.trainset capping values
for (i in 1:ncol(gbm.testset)){
  if ((substr(colnames(gbm.testset)[i],1,5)=='Weat_'|substr(colnames(gbm.testset)[i],1,5)=='HC01_')&(class(eval(parse(text=paste0('gbm.testset$',colnames(gbm.testset)[i]))))=='numeric'|class(eval(parse(text=paste0('gbm.testset$',colnames(gbm.testset)[i]))))=='integer')){
    gbm.testset[,paste0(colnames(gbm.testset)[i])]<-pmin(pmax(gbm.testset[,i],capping.gbm[1,colnames(gbm.testset)[i]]),capping.gbm[2,colnames(gbm.testset)[i]])
  }
}

#Convert character variables to factors
for (i in 1:ncol(gbm.testset)){
  if((class(gbm.testset[,i])=='character')){   
    gbm.testset[,i]<-as.factor(gbm.testset[,i])
  }
}

#select useful columns 
gbm.testset<-as.data.frame(gbm.testset%>%select(Target,starts_with('Weat_'),starts_with('flag_'),starts_with('HC01_'),STATE.x,ELEVATION))

```

####Run gradient boosting model \newline

As parameter tuning is time-consuming, R markdown file documented the codes as below, however, cross-validation parameter tuning was run separately from this file with the following codes.
```{r, echo=T, results='hide', warning=FALSE, message=FALSE}
# ctrl <- trainControl(method = "repeatedcv",
#                      number = 5,
#                      # repeats = 5,
#                      summaryFunction=defaultSummary)
# 
# 
# set.seed(5627)
# 
# gbmGrid <-  expand.grid(interaction.depth = c(2,3),
#                         n.trees = (1:3)*500, 
#                         shrinkage = c(0.1),
#                         n.minobsinnode = 100) 
# 
# gbm_fit <- train(Target~.,
#                  data = trainset,
#                  method = "gbm",
#                  bag.fraction = 0.5,
#                  verbose = FALSE,
#                  metric = "RMSE",
#                  trControl = ctrl,
#                  tuneGrid =gbmGrid
# )
# 
# plot(gbm_fit)
# summary(gbm_fit)
# gbm_fit$bestTune

#run gbm with tuned parameters##############
set.seed(12345)
gbm_tuned <- gbm(Target~.,
                 distribution = "gaussian",
                 data = gbm.trainset,
                 n.trees = 1500,# convergence issue, just need enough to converge
                 shrinkage = 0.1,  # convergence issue, lower is better but takes longer.
                 interaction.depth = 3, # effects tree complexity
                 n.minobsinnode = 200, # tree argument 1082/bag.fraction/train.fraction
                 bag.fraction = 0.5, #optimization argument
                 train.fraction = 1.0,
                 cv.folds=5, #gives condition on how good a model is doing
                 keep.data = TRUE,
                 verbose = TRUE, #"CV",
                 class.stratify.cv=NULL
)
best.iter <- gbm.perf(gbm_tuned, method="cv")

```

####Gains Chart
```{r warning=FALSE, message=FALSE}
#Predict on trainset and testset
gbm.trainset$gbm_pred<- predict.gbm(gbm_tuned, n.trees = best.iter,newdata = gbm.trainset)
gbm.testset$gbm_pred <- predict.gbm(gbm_tuned, n.trees = best.iter,newdata = gbm.testset)

#Gains Chart on testset
GAINS.CHART(gbm.testset$Target,gbm.testset$gbm_pred,n.rank=20)
```

####Lorenz Curve, Train vs Test
```{r, warning=FALSE, message=FALSE}
##Lorenz Curve
trainCheck<- OrderedCDF(Score = gbm.trainset$gbm_pred, Loss = (gbm.trainset$Target), Weight =rep(1,nrow(gbm.trainset)), NBins = 1000)
ValidationCheck<- OrderedCDF(Score = gbm.testset$gbm_pred, Loss = (gbm.testset$Target), Weight =rep(1,nrow(gbm.testset)), NBins = 1000)

ValidationWeight <- ValidationCheck$WeightPts
ValidationLoss <- ValidationCheck$LossPts
TrainWeight <- trainCheck$WeightPts
TrainLoss <- trainCheck$LossPts

ModelComparisonData <- data.frame(ValidationWeight, ValidationLoss, TrainWeight, TrainLoss)

```

```{r warning=FALSE, message=FALSE, fig.height=4,fig.width=6}
ModelComparisonData %>% ggvis(~ValidationWeight, ~ValidationLoss) %>% layer_paths(stroke := "blue") %>% 
  layer_paths(data = ModelComparisonData, x = ~ValidationWeight, y = ~ValidationWeight, stroke := "black") %>%
  layer_paths(data = ModelComparisonData, x = ~TrainWeight, y = ~TrainLoss, stroke := "red")
```

###6. Conclusion
Based on the analysis, we see a significant relationship between weather related attributes and home pices. We can make some preliminary conclusions based on the analysis: \newline

1. Home price is higher when precipitation is lower, which is intuitive, as high summer precipitation may cause flood. People prefer cooler weather in summer, which drives housing demand, and increase the home price, which is consistent with our results that home price is higher when summer tempurature is lower. Based on the analysis, winter tempurature does not have as big of an impact to the home price as summer tempurater. \newline

2. Geographic location has a big impace on home price, which is also intuitive. This project used 'State' to model the impact of geographic location on home price. For future improvement, regional factors can be considered as the U.S. is typically grouped into climate regions based on similar weather patterns.  \newline

3. By comparing different models, gradient boosting model is our chosen model for this project, as it performed better than liner regression with better gains chart, and higher KS score. Gradient boosing was able to catch the non-linear relationship between our independent variables and the dependent variable, such as precipitation. Also gradient boosting does not require as much feature engineering as linear regression, which is ideal for a time-constrained project. Binning and smoothing can help to address some non-linear relationships in linear regression. However, the project only looked into weather related variables for non-linear relationship transformation, which can be improved in future study.

###7. Next Step
1. Segmentation based on geographic regions

2. Linear regression binning and smoothing

3. Missing value imputation with multiple imputation

4. Test the results on Holdout dataset.
 



